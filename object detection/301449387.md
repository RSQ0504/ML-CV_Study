# Project3 Report

### Rongsheng Qian 301449387

### Team: R.S Qian&Yuwen Jia&Isaac Ding

### Names of group members: Rongsheng Qian, Isaac Ding, Yuwen Jia SFU

# <span style="color:red;">Using 3 free late day</span>

### Best accuracy: 0.42208













































# Part 1

## 1.1 List of the configs and modifications that you used.

### 1.1.1 Configs

```python
cfg = get_cfg()
cfg.OUTPUT_DIR = "{}/output_custom/".format(BASE_DIR)

cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/retinanet_R_101_FPN_3x.yaml"))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/retinanet_R_101_FPN_3x.yaml")

cfg.DATASETS.TRAIN = ("custom_train",)
cfg.DATASETS.TEST = ("custom_val",)
cfg.DATALOADER.NUM_WORKERS = 2

cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.0001
cfg.SOLVER.MAX_ITER = 1000

cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5
```

### 1.1.2 Create 3 Dataloader (Data augmentation) (train 3 times using 3 CustomTrainer3)

```python
class CustomDatasetMapper1(DatasetMapper):
    def __init__(self, cfg, is_train=True):
        super().__init__(cfg, is_train)
        augmentations = []
        if is_train:
            augmentations.append(T.Resize((800, 800)))
            augmentations.append(T.RandomRotation([-5,5]))
        self.augmentations = T.AugmentationList(augmentations)

    def __call__(self, dataset_dict):
        dataset_dict = super().__call__(dataset_dict)
        return dataset_dict


class CustomDatasetMapper2(DatasetMapper):
    def __init__(self, cfg, is_train=True):
        super().__init__(cfg, is_train)
        augmentations = []
        if is_train:
            augmentations.append(T.Resize((800, 800)))
            augmentations.append(T.RandomFlip(prob=1))
        self.augmentations = T.AugmentationList(augmentations)

    def __call__(self, dataset_dict):
        dataset_dict = super().__call__(dataset_dict)
        return dataset_dict

class CustomDatasetMapper3(DatasetMapper):
    def __init__(self, cfg, is_train=True):
        super().__init__(cfg, is_train)
        augmentations = []
        if is_train:
            augmentations.append(T.Resize((800, 800)))

        self.augmentations = T.AugmentationList(augmentations)

    def __call__(self, dataset_dict):
        dataset_dict = super().__call__(dataset_dict)
        return dataset_dict

      
class CustomTrainer1(DefaultTrainer):
    @classmethod
    def build_train_loader(cls, cfg):
        return build_detection_train_loader(cfg, mapper=CustomDatasetMapper1(cfg, is_train=True))

class CustomTrainer2(DefaultTrainer):
    @classmethod
    def build_train_loader(cls, cfg):
        return build_detection_train_loader(cfg, mapper=CustomDatasetMapper2(cfg, is_train=True))

class CustomTrainer3(DefaultTrainer):
    @classmethod
    def build_train_loader(cls, cfg):
        return build_detection_train_loader(cfg, mapper=CustomDatasetMapper3(cfg, is_train=True))
```

### 1.1.3 **Data Processing.**

**Divide an image into smaller blocks (800*800) if the bounding box is less than 50% of its total area in this block, I drop it**

```python
def is_bbox_in_area(bbox_xywh,area_xyxy):
    bbox_x1 = bbox_xywh[0]
    bbox_y1 = bbox_xywh[1]
    bbox_x2 = bbox_xywh[0] + bbox_xywh[2]
    bbox_y2 = bbox_xywh[1] + bbox_xywh[3]

    bbox = box(bbox_x1, bbox_y1, bbox_x2, bbox_y2)
    area = box(area_xyxy[0], area_xyxy[1], area_xyxy[2], area_xyxy[3])
    intersect = bbox.intersection(area)

    intersect_area = intersect.area
    bbox_area = bbox.area
    threshold_area = 0.5 * bbox_area

    if threshold_area > intersect_area:
        return []

    if not intersect.is_empty:
        x1, y1, x2, y2 = intersect.bounds
        w = x2 - x1
        h = y2 - y1
        return [x1,y1,w,h]
    else:
        return []
```

### 1.1.4 Add validation set

**Set 19 images as validation set which is 10% of whole training set**

## 1.2 Factors which helped improve the performance. 

### 1.2.1 Increase the size of val set from 8 images to 19 images

**The val accuracy increased from baseline 23% to 28%**

<img src="/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.57.16 AM.png" alt="Screenshot 2023-11-10 at 10.57.16 AM" style="zoom:50%;" />

### 1.2.2 Data Processing.

**The val accuracy increased from baseline 29% to 43%**

### 1.2.2.1  if the bounding box is less than 50% of its total area in this block, I drop it. Comparing with the bounding box is less than 30% of its total area in this block

**less than 50%**![part1 0.5](/Users/davidqian/Desktop/Cache/part1 0.5.png)

**less than 30%**![part1 0.3](/Users/davidqian/Desktop/Cache/part1 0.3.png)

**This factor can increase the accuracy from 26% to 31%.**

### 1.2.3 Using `retinanet_R_101_FPN_3x.yaml` model and pre-train instead of `faster_rcnn_R_101_FPN_3x.yaml`

**This factors can increase val accuracy from 60% to 63%**

### 1.2.4 Create 3 Dataloader (Data augmentation) (original images, filping images, rotation images)

**This factors can increase val accuracy from 63% to 67%**

![Screenshot 2023-11-10 at 10.39.12 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.39.12 PM.png)

![Screenshot 2023-11-10 at 11.05.25 AM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 11.05.25 AM.png)

## 1.3 Final plot for total training loss and accuracy.

### 1.3.1 final accuracy:

![Screenshot 2023-11-10 at 11.05.25 AM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 11.05.25 AM.png)

### 1.3.2 Plot

![Screenshot 2023-11-10 at 11.27.37 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 11.27.37 PM.png)



![Screenshot 2023-11-10 at 11.27.10 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 11.27.10 PM.png)

## 1.4 The visualization of 3 samples from the test set and the predicted results.

![Screenshot 2023-11-10 at 10.47.51 AM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.47.51 AM.png)

![Unknown-2](/Users/davidqian/Downloads/Unknown-2.png)

<img src="/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.48.09 AM.png" alt="Screenshot 2023-11-10 at 10.48.09 AM" style="zoom:50%;" />

## 1.5 Ablation Study

### **1.5.1 Divide an image into smaller blocks (800*800) ** if the bounding box is less than 50% of its total area in this block drop it 

**This factor can increase the accuracy from 28% to 64%.**

**The small plane can be detected more easily. The visualization is shown below. The origin model can not detect the small plane in this sample, however the model after adding this factor can detect it with a acceptable precision.**  

**Droping the bounding box which is less than 50% of its total area can help model to recognize how the plane looks like rather than predicted a building as a plane.**

**Without the Data Processing**:

![Unknown-3](/Users/davidqian/Downloads/Unknown-3.png)

<img src="/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.57.16 AM.png" alt="Screenshot 2023-11-10 at 10.57.16 AM" style="zoom:67%;" />

**With the Data Processing:**

![Unknown-2](/Users/davidqian/Downloads/Unknown-2.png)

![Screenshot 2023-11-10 at 10.39.12 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.39.12 PM.png)



### **1.5.2 Data augmentation**

**The image visualization result doesn’t have the obvious difference.**

**However this factors can increase val accuracy from 63% to 67%**

**Data augmentation can increase the training set and let the network learn more features, which can increase the subtability of model in different kinds of images.**

![Screenshot 2023-11-10 at 10.39.12 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 10.39.12 PM.png)

![Screenshot 2023-11-10 at 11.05.25 AM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 11.05.25 AM.png)

# Part 2

## 2.1 Any hyperparameter settings I used 

```python
num_epochs = 70
batch_size = 4
learning_rate = 0.01
weight_decay = 1e-5
```

## 2.2  The final architecture of my network 

### 2.2.1 Modification

##### Increasing channels layers using conv2d: (3,16), (16, 32), (64,128), (128,256), (256,512)

##### Repeated layers (in_channel == out_channel) using conv2d after (16,32), (32,64), (64,128), (128,256), there isn’t repeated layer after (3,16)

##### Added fc layer between the encoder and decoder

### 2.2.2 explain the reason for each modification

### **Repeating the CONV with the same number of input channels and output channels**

##### Each repeating convolutional layer can capture different features, which can let network learn more complex features. It works when the train data is limited, which can mitigate overfitting issues

#####  It can increase the depth of the network and add more non-linear part in network.

### 2.2.3  Final architecture

```python
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()

        # Encoder

        self.input_conv = conv(3, 16)
        self.down = nn.Sequential(
            down(16, 32),
            conv(32, 32),
            conv(32, 32),

            down(32, 64),
            conv(64, 64),
            conv(64, 64),

            down(64,128),
            conv(128, 128),
            conv(128, 128),

            down(128,256),
            conv(256, 256),
            conv(256, 256),

            down(256,512)
        )

        self.fc_net = nn.Sequential(
            nn.Linear(512*4*4, 512*4*4),
            nn.BatchNorm1d(512*4*4),
            nn.ReLU(inplace=True)
        )

        # Decoder

        self.up = nn.Sequential(
            up(512, 256),
            conv(256, 256),
            conv(256, 256),

            up(256,128),
            conv(128, 128),
            conv(128, 128),

            up(128,64),
            conv(64, 64),
            conv(64, 64),

            up(64,32),
            conv(32, 32),
            conv(32, 32),

            up(32,16)
        )
        self.output_conv = conv(16, 1, False) # ReLu activation is removed to keep the logits for the loss function


    def forward(self, input):
      y = self.input_conv(input)
      y = self.down(y)

      b,c,h,w = y.size(0),y.size(1),y.size(2),y.size(3)
      y = y.view(b, -1)
      y = self.fc_net(y)
      y = y.view(b,c,h,w)

      y = self.up(y)
      output = self.output_conv(y)
      #print(output.shape)
      return output
```



## 2.3 Report the loss functions that I used and the plot the total training loss of the training procedure

### 2.3.1 Loss function I used

```python
crit = nn.BCEWithLogitsLoss() # Define the loss function
optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
```

### 2.3.2 training loss

**My training loss is 3 times than the original one is becuase I did data augmentation inside the for loop, which have orginal image, h_filp image and v_filp image**

```python
loss_list = [0.9716119170188904,0.7413926720619202,0.6809871792793274,0.6373889446258545,0.6082096695899963,0.5845683217048645,0.5660731196403503,0.5509427785873413,
                0.55094278,0.54155553,0.53216827,0.52278102,0.51339377,0.50400652,0.49461927,0.48523201,0.47584476,0.46645751,0.45707026,0.447683,0.43829575,
                0.43829575181007385,0.4328637719154358,0.428557425737381,0.42350563406944275,0.4198061525821686,0.41460368037223816,0.41136375069618225, 0.4045928120613098,
                0.4032733142375946,0.39848583936691284,0.39379405975341797,0.3937068283557892,0.3881424069404602,0.3862606883049011,0.38249462842941284,
                0.37887489795684814, 0.3757827877998352, 0.3725910782814026, 0.36895090341567993, 0.3667604923248291, 0.36521628499031067,0.36099541187286377,0.35789838433265686,
                0.35567939281463623,0.35227134823799133,0.35101622343063354,0.3480479121208191,0.34603017568588257,0.3448733687400818, 0.3411721885204315, 0.3396941125392914, 
                0.3374038338661194, 0.33560723066329956, 0.33230581879615784, 0.33006858825683594, 0.3282719850540161, 0.3272158205509186, 0.32439732551574707, 0.32234859466552734, 0.3214151859283447, 0.3203503489494324,
                0.3174724280834198, 0.31584280729293823, 0.3132993280887604, 0.31308993697166443, 0.31090065836906433, 0.3083840012550354, 0.3057791590690613]

```

<img src="/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 3.24.19 PM.png" alt="Screenshot 2023-11-10 at 3.24.19 PM" style="zoom:50%;" />

## 2.4 Report the final mean IoU of my model.

<img src="/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 8.41.21 PM.png" alt="Screenshot 2023-11-10 at 8.41.21 PM"  />

## 2.5 Visualize 3 images from the test set and the corresponding predicted masks.

<img src="/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 8.43.31 PM.png" alt="Screenshot 2023-11-10 at 8.43.31 PM" style="zoom: 67%;" />![Screenshot 2023-11-10 at 8.43.41 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 8.43.41 PM.png)![Screenshot 2023-11-10 at 8.43.50 PM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-10 at 8.43.50 PM.png)

# Part3

## 3.1 Names of my group members

### Team Member: Rongsheng Qian & Yuwen Jia & Isaac Ding

### Names of group members: Rongsheng Qian, Isaac Ding, Yuwen Jia

## 3.2 Report the best score

### 0.42208

## 3.3 The visualization of results for 3 random samples from the test set.

![Unknown](/Users/davidqian/Desktop/Cache/Unknown.png)

![Unknown-2](/Users/davidqian/Desktop/Cache/Unknown-2.png)

![Unknown-3](/Users/davidqian/Desktop/Cache/Unknown-3.png)

![Unknown-4](/Users/davidqian/Desktop/Cache/Unknown-4.png)

![Unknown-5](/Users/davidqian/Desktop/Cache/Unknown-5.png)

![Unknown-6](/Users/davidqian/Desktop/Cache/Unknown-6.png)

# Part4

## 4.1 The visualization and the evaluation results similar to Part 1.

![Unknown copy 2](/Users/davidqian/Desktop/Cache/Unknown-2 copy.png)

<img src="/Users/davidqian/Desktop/Cache/Unknown copy 2.png" alt="Unknown copy 2" style="zoom:50%;" />

![Unknown](/Users/davidqian/Downloads/Unknown.png)

## 4.2 Explain the differences between the results of Part 3 and Part 4

![Screenshot 2023-11-11 at 12.03.08 AM](/Users/davidqian/Desktop/Cache/Screenshot 2023-11-11 at 12.03.08 AM.png)

### 4.2.1 Training & inference time

**Its training & inference time is potentially slower to converge than models focused solely on object detection due to its dual-task nature.**

### 4.2.2 **Flexibility**

**Part 3 model have more flexibility than part 4, which can adapt model to specific task requirements. For example, the mask-rcnn cannot find the plane in last sample but the combination of RetinaNet and FCN can do it by adding post-processing and pre-processing.**

### 4.2.3 Segmentation

**The result of segmentation in part4 seems worse than part3, which cannot recognize the whole plane. Its Ap of segm only have 8.4 which is lower than the part 3**